"Model","Year","CO2_per_Parameter","Energy_Efficiency","Organization"
"AlexNet",2017,83.33,0.0012,"University of Toronto"
"BERT",2018,102.94,9e-04,"Google"
"GPT-2",2019,80,0.0013,"OpenAI"
"Megatron-LM",2019,60.24,0.0014,"NVIDIA"
"T5",2020,72.73,0.0012,"Google"
"GPT-3",2020,3.15,1e-04,"OpenAI"
"Switch Transformer",2021,0.59,8e-04,"Google"
"Jurassic-1",2021,3.37,1e-04,"AI21 Labs"
"Codex",2021,20.83,0,"OpenAI"
"BLOOM",2022,2.46,2e-04,"BigScience"
"Gopher",2022,2.68,1e-04,"DeepMind"
"Chinchilla",2022,6.86,1e-04,"DeepMind"
"PaLM",2022,2.22,2e-04,"Google"
"OPT",2022,2.46,2e-04,"Meta"
"LLaMA",2023,5.54,1e-04,"Meta"
"GPT-4",2023,5.18,3e-04,"OpenAI"
"Falcon-180B",2023,2.83,2e-04,"TII UAE"
"Gemini 1.0",2023,3.33,2e-04,"Google DeepMind"
"LLaMA 3.1",2024,22.05,2e-04,"Meta"
"Gemini 1.5",2024,1.79,3e-04,"Google DeepMind"
"Claude 3 Opus",2024,2.62,2e-04,"Anthropic"
"GPT-5 (est.)",2025,3.6,3e-04,"OpenAI"
"Claude 4 (est.)",2025,3.5,3e-04,"Anthropic"
"Gemini 2 (est.)",2025,2.86,3e-04,"Google DeepMind"
