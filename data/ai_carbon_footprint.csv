"Model","Year","Parameters_Billion","EnergyUsed_MWh","CO2_Tons","Organization","Training_Location","Energy_Efficiency","CO2_per_Parameter"
"AlexNet",2017,0.06,50,5,"University of Toronto","Canada",0.0012,83.33
"BERT",2018,0.34,400,35,"Google","USA",9e-04,102.94
"GPT-2",2019,1.5,1200,120,"OpenAI","USA",0.0013,80
"Megatron-LM",2019,8.3,5800,500,"NVIDIA","USA",0.0014,60.24
"T5",2020,11,9400,800,"Google","USA",0.0012,72.73
"GPT-3",2020,175,1300000,552,"OpenAI","USA",1e-04,3.15
"Switch Transformer",2021,1600,2100000,950,"Google","USA",8e-04,0.59
"Jurassic-1",2021,178,1200000,600,"AI21 Labs","Israel",1e-04,3.37
"Codex",2021,12,4e+05,250,"OpenAI","USA",0,20.83
"BLOOM",2022,176,1082000,433,"BigScience","France",2e-04,2.46
"Gopher",2022,280,1900000,750,"DeepMind","UK",1e-04,2.68
"Chinchilla",2022,70,1e+06,480,"DeepMind","UK",1e-04,6.86
"PaLM",2022,540,2500000,1200,"Google","USA",2e-04,2.22
"OPT",2022,175,960000,430,"Meta","USA",2e-04,2.46
"LLaMA",2023,65,9e+05,360,"Meta","USA",1e-04,5.54
"GPT-4",2023,1000,3800000,5184,"OpenAI","USA",3e-04,5.18
"Falcon-180B",2023,180,1100000,510,"TII UAE","UAE",2e-04,2.83
"Gemini 1.0",2023,600,3500000,2000,"Google DeepMind","USA",2e-04,3.33
"LLaMA 3.1",2024,405,1700000,8930,"Meta","USA",2e-04,22.05
"Gemini 1.5",2024,1200,4200000,2150,"Google DeepMind","USA",3e-04,1.79
"Claude 3 Opus",2024,400,2200000,1050,"Anthropic","USA",2e-04,2.62
"GPT-5 (est.)",2025,2000,6500000,7200,"OpenAI","USA",3e-04,3.6
"Claude 4 (est.)",2025,1000,3800000,3500,"Anthropic","USA",3e-04,3.5
"Gemini 2 (est.)",2025,2200,7e+06,6300,"Google DeepMind","USA",3e-04,2.86
